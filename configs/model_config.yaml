# Model Configuration for Fine-tuning Approach

# Supported models (as recommended in README)
models:
  # Baseline model - strong performance on Webis-Clickbait-17
  roberta_base:
    model_name: "roberta-base"
    tokenizer_name: "roberta-base"
    max_length: 512
    num_labels: 2
    architecture: "RobertaForSequenceClassification"
    
  # Advanced models for modern approach
  mistral_7b:
    model_name: "mistralai/Mistral-7B-v0.1"
    tokenizer_name: "mistralai/Mistral-7B-v0.1"
    max_length: 512  # Can be extended to 4096+ if needed
    num_labels: 2
    architecture: "MistralForSequenceClassification"
    use_flash_attention: true
    
  llama3_8b:
    model_name: "meta-llama/Meta-Llama-3-8B"
    tokenizer_name: "meta-llama/Meta-Llama-3-8B"
    max_length: 512
    num_labels: 2
    architecture: "LlamaForSequenceClassification"
    use_flash_attention: true
    
  # Lightweight alternatives
  distilbert:
    model_name: "distilbert-base-uncased"
    tokenizer_name: "distilbert-base-uncased"
    max_length: 512
    num_labels: 2
    architecture: "DistilBertForSequenceClassification"

# Model-specific configurations
model_configs:
  # Quantization settings (for memory efficiency)
  quantization:
    load_in_4bit: true
    load_in_8bit: false
    bnb_4bit_use_double_quant: true
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_compute_dtype: "bfloat16"
    
  # Memory optimization
  memory_optimization:
    gradient_checkpointing: true
    dataloader_pin_memory: false
    remove_unused_columns: true
    
# Tokenization settings
tokenization:
  # Common settings
  padding: "max_length"
  truncation: true
  return_tensors: "pt"
  
  # Special tokens handling
  add_special_tokens: true
  return_attention_mask: true
  return_token_type_ids: false  # Not needed for RoBERTa
  
  # Text preprocessing before tokenization
  clean_up_tokenization_spaces: true

# Model loading settings
loading:
  # Trust remote code (for some models)
  trust_remote_code: true
  
  # Device placement
  device_map: "auto"  # Automatic device placement
  
  # Low CPU memory usage
  low_cpu_mem_usage: true
  
  # Torch dtype
  torch_dtype: "auto"  # Will use model's default

# Output settings
output:
  # Where to save fine-tuned models
  save_directory: "models/fine_tuned"
  
  # Checkpointing
  save_total_limit: 3
  save_strategy: "steps"
  save_steps: 500
  
  # Model pushing to HuggingFace Hub (optional)
  push_to_hub: false
  hub_model_id: null
  hub_token: null 
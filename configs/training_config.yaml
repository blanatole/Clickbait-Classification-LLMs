# Training Configuration for Fine-tuning

# Basic training parameters
training:
  # Training strategy
  num_train_epochs: 3
  per_device_train_batch_size: 16  # Adjust based on GPU memory
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 1
  
  # Learning rate and optimization
  learning_rate: 2e-5
  weight_decay: 0.01
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  
  # Learning rate scheduling
  lr_scheduler_type: "cosine"
  warmup_steps: 500
  warmup_ratio: 0.1
  
  # Evaluation and checkpointing
  evaluation_strategy: "steps"
  eval_steps: 500
  logging_steps: 100
  save_steps: 500
  
  # Early stopping
  early_stopping_patience: 3
  metric_for_best_model: "f1"
  greater_is_better: true
  
  # Model selection
  load_best_model_at_end: true

# PEFT/LoRA Configuration (as recommended in README)
peft:
  use_peft: true
  
  # LoRA settings
  lora:
    r: 16              # Rank of adaptation
    lora_alpha: 32     # LoRA scaling parameter
    lora_dropout: 0.1  # LoRA dropout
    bias: "none"       # Bias type
    task_type: "SEQ_CLS"  # Sequence classification
    
    # Target modules for different architectures
    target_modules:
      roberta: ["query", "value", "key", "dense"]
      mistral: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      llama: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      distilbert: ["q_lin", "v_lin", "k_lin", "out_lin"]

# Unsloth optimization (for Mistral/LLaMA)
unsloth:
  use_unsloth: false  # Set to true when using Mistral/LLaMA
  
  # Unsloth-specific settings
  max_seq_length: 512
  dtype: "float16"  # or "bfloat16"
  load_in_4bit: true

# Data loading
data_loading:
  dataloader_num_workers: 4
  dataloader_pin_memory: false  # Set to false for Windows compatibility
  remove_unused_columns: true
  
  # Data collator
  data_collator: "DataCollatorWithPadding"

# Optimization settings for different hardware
hardware_optimization:
  # For A5000 GPU (when available)
  a5000:
    per_device_train_batch_size: 32
    per_device_eval_batch_size: 64
    gradient_accumulation_steps: 1
    fp16: false
    bf16: true  # Better for A5000
    dataloader_pin_memory: true
    
  # For weaker hardware (current setup)
  cpu_fallback:
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 8
    gradient_accumulation_steps: 4
    fp16: false
    bf16: false
    dataloader_pin_memory: false
    
  # For general GPU
  gpu_general:
    per_device_train_batch_size: 16
    per_device_eval_batch_size: 32
    gradient_accumulation_steps: 2
    fp16: true
    bf16: false
    dataloader_pin_memory: true

# Logging and monitoring
logging:
  # Local logging
  logging_dir: "outputs/logs"
  report_to: []  # Can add "wandb", "tensorboard"
  
  # Weights & Biases (optional)
  wandb:
    project: "clickbait-detection"
    entity: null  # Your wandb username
    name: null    # Run name (will be auto-generated)
    
  # TensorBoard
  tensorboard:
    log_dir: "outputs/tensorboard"

# Model comparison settings
comparison:
  # Metrics to track
  metrics:
    - "accuracy"
    - "precision"
    - "recall" 
    - "f1"
    
  # Benchmark targets (from README)
  benchmark_targets:
    # Clickbait Challenge 2017 winner
    goldfish_accuracy: 0.876
    goldfish_f1: 0.741
    
    # Modern Transformer baselines
    roberta_accuracy: 0.8575
    roberta_f1: 0.6901
    
  # Performance thresholds
  minimum_f1: 0.70
  target_f1: 0.75

# Reproducibility
reproducibility:
  seed: 42
  deterministic: true
  
# Output paths
output:
  output_dir: "outputs/fine_tuning"
  logging_dir: "outputs/logs"
  checkpoint_dir: "outputs/checkpoints" 
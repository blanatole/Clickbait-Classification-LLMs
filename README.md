# ğŸ¯ Clickbait Classification Fine-Tuning

Dá»± Ã¡n fine-tune cÃ¡c mÃ´ hÃ¬nh Transformers (BERT, DeBERTa, PhoBERT, LLaMA/Mistral + LoRA) cho bÃ i toÃ¡n phÃ¢n loáº¡i clickbait trÃªn táº­p dá»¯ liá»‡u Webis-Clickbait-17.

## ğŸ“‚ Cáº¥u trÃºc Project

```
clickbait-classification/
â”œâ”€â”€ ğŸ“Š data/                      # Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c chia sáºµn
â”‚   â”œâ”€â”€ train/data.jsonl         # 30,812 máº«u training
â”‚   â”œâ”€â”€ val/data.jsonl           # Validation set  
â”‚   â””â”€â”€ test/data.jsonl          # Test set
â”œâ”€â”€ ğŸš€ scripts/                   # Training & evaluation scripts
â”‚   â”œâ”€â”€ train_deberta.py         # Fine-tune DeBERTa-v3-base
â”‚   â”œâ”€â”€ train_lora.py            # Fine-tune vá»›i LoRA
â”‚   â”œâ”€â”€ evaluate_model.py        # ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
â”‚   â”œâ”€â”€ inference.py             # Inference script
â”‚   â””â”€â”€ setup_environment.py     # Kiá»ƒm tra mÃ´i trÆ°á»ng
â”œâ”€â”€ ğŸ”§ utils/                     # Utility functions (legacy)
â”‚   â”œâ”€â”€ utils.py                 # General utilities
â”‚   â”œâ”€â”€ data_preprocessor.py     # Data preprocessing
â”‚   â””â”€â”€ data_analysis.py         # Data analysis tools
â”œâ”€â”€ ğŸ“š docs/                      # Documentation
â”‚   â””â”€â”€ FINE_TUNING_GUIDE.md     # HÆ°á»›ng dáº«n chi tiáº¿t
â”œâ”€â”€ âš™ï¸ configs/                   # Configuration files
â”‚   â””â”€â”€ model_configs.py         # Model configurations
â”œâ”€â”€ ğŸ“ˆ outputs/                   # Training outputs
â”‚   â”œâ”€â”€ checkpoints/             # Model checkpoints
â”‚   â””â”€â”€ logs/                    # Training logs
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ README.md                    # This file
â””â”€â”€ .gitignore                   # Git ignore rules
```

## ğŸš€ Quick Start

### For RTX A5000 Users (Recommended)

```bash
# Interactive training guide with all optimized models
python scripts/quick_start_a5000.py
```

### Manual Training

#### 1. Check environment
```bash
python scripts/setup_environment.py
```

#### 2. Train specific models
```bash
# BERT family models (optimized batch sizes)
python scripts/train_bert_family.py --model bert-base-uncased
python scripts/train_bert_family.py --model deberta-v3-base
python scripts/train_bert_family.py --model all

# Large Language Models with QLoRA
python scripts/train_llm_lora.py --model mistral-7b-v0.2
python scripts/train_llm_lora.py --model llama3-8b
python scripts/train_llm_lora.py --model all
```

#### 3. Run full benchmark suite
```bash
python scripts/run_all_experiments.py
```

#### 4. Generate comparison results
```bash
python scripts/benchmark_results.py --save_csv
```

## ğŸ“Š Káº¿t quáº£ mong Ä‘á»£i

| Model | F1-Score | Accuracy | Training Time | VRAM |
|-------|----------|----------|---------------|------|
| DeBERTa-v3-base | **0.72** | **86%** | 45 min | 8 GB |
| LoRA (DialoGPT) | 0.68 | 82% | 25 min | 6 GB |
| LoRA (Mistral-7B) | **0.75** | **88%** | 90 min | 20 GB |

## ğŸ”§ YÃªu cáº§u há»‡ thá»‘ng

- **Python**: 3.8+
- **GPU**: CUDA-compatible (khuyáº¿n nghá»‹)
  - BERT/DeBERTa: â‰¥ 8 GB VRAM
  - LoRA 7B: â‰¥ 16-24 GB VRAM
- **RAM**: â‰¥ 16 GB
- **Storage**: â‰¥ 10 GB free space

## ğŸ“‹ Dependencies

```bash
pip install -r requirements.txt
```

**Core libraries:**
- `torch>=2.1.0` - PyTorch
- `transformers>=4.35.0` - Hugging Face Transformers
- `datasets>=2.14.0` - Dataset handling
- `peft>=0.6.0` - LoRA implementation
- `scikit-learn>=1.3.0` - Metrics vÃ  evaluation

## ğŸ¯ Usage Examples

### Training vá»›i custom parameters

```python
# Trong train_deberta.py, tÃ¹y chá»‰nh:
training_args = TrainingArguments(
    learning_rate=1e-5,           # Giáº£m learning rate
    per_device_train_batch_size=8, # Giáº£m batch size náº¿u thiáº¿u VRAM
    num_train_epochs=3,           # Ãt epochs hÆ¡n
    fp16=True,                    # Mixed precision
)
```

### Inference trÃªn text má»›i

```python
from transformers import pipeline

classifier = pipeline(
    "text-classification",
    model="outputs/deberta-v3-clickbait"
)

text = "Báº¡n sáº½ khÃ´ng tin Ä‘iá»u xáº£y ra tiáº¿p theo..."
result = classifier(text)
print(result)  # [{'label': 'LABEL_1', 'score': 0.95}]
```

## ğŸ“š Documentation

Xem **[docs/FINE_TUNING_GUIDE.md](docs/FINE_TUNING_GUIDE.md)** Ä‘á»ƒ cÃ³ hÆ°á»›ng dáº«n chi tiáº¿t tá»«ng bÆ°á»›c.

## ğŸ¤ Contributions

Má»i contributions Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n! HÃ£y:
- Report bugs
- Suggest improvements  
- Add new features
- Share your training results

## ğŸ“„ License

This project is licensed under the MIT License.

---

**Happy Fine-tuning! ğŸ‰**

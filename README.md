# Clickbait Classification using LLM Fine-tuning

A complete pipeline for detecting clickbait headlines using fine-tuned BERT-family models and LoRA/QLoRA-adapted large language models. Optimized for high-end CUDA GPUs but configurable for various hardware setups.

## ğŸ“‘ Table of Contents

- [ğŸ¯ Project Overview](#-project-overview)
- [ğŸ“Š Dataset](#-dataset)
- [ğŸ› ï¸ Installation & Setup](#ï¸-installation--setup)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ¤– Supported Models](#-supported-models)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“Š Performance Results](#-performance-results)
- [ğŸ”§ Technical Fixes](#-technical-fixes-implemented)
- [ğŸ› Troubleshooting](#-troubleshooting)
- [ğŸ“ˆ Monitoring & Logging](#-monitoring--logging)
- [ğŸ” Model Configuration Details](#-model-configuration-details)
- [ğŸ¯ Future Improvements](#-future-improvements)
- [ğŸ“‹ Requirements](#-requirements)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸš€ Push to GitHub (Repo Owner)](#-push-to-github-repo-owner)
- [ğŸ“„ License](#-license)
- [ğŸ™ Acknowledgments](#-acknowledgments)
- [ğŸ“ Support](#-support)

## ğŸ¯ Project Overview

This repository implements two complementary approaches to classifying Twitter headlines as clickbait or not-clickbait using the Webis-Clickbait-17 corpus:

### ğŸ”¬ Fine-tuning Module
Complete pipeline for fine-tuning models with your own data:
- **BERT family models**: Full fine-tuning of BERT-base, BERT-large, RoBERTa
- **Large Language Models**: Parameter-efficient fine-tuning with LoRA/QLoRA for Mistral, Llama models
- **Comprehensive evaluation**: Metrics, analysis, and model comparison tools

### ğŸ¯ Prompting Module  
Zero-shot and few-shot approaches using pre-trained models:
- **Zero-shot prompting**: Direct classification without training
- **Few-shot prompting**: With example demonstrations
- **Advanced techniques**: Chain-of-thought, self-consistency, improved prompting strategies
- **Multiple model support**: OpenAI GPT, Claude, local models via API

### ğŸ”§ Shared Resources
Common utilities and resources used by both approaches:
- **Data preprocessing**: Standardized data handling and analysis
- **Evaluation framework**: Unified metrics and benchmarking
- **Environment setup**: Automated dependency management
- **Comparison tools**: Side-by-side performance analysis

All scripts are optimized for high-end CUDA GPUs but configurable for smaller GPUs through CLI parameters.

## ğŸ“Š Dataset 

- **Source**: Webis-Clickbait-17 dataset
- **Total samples**: 38,517 Twitter headlines
- **Split**: 
  - Train: 30,812 samples
  - Validation: 3,851 samples  
  - Test: 3,854 samples
- **Labels**: Binary classification (0: non-clickbait, 1: clickbait)
- **Format**: JSONL files with `text` and `label` fields

## ğŸ› ï¸ Installation & Setup

### Prerequisites
- Python 3.10+
- CUDA-compatible GPU (high-end GPU recommended)
- 16GB+ VRAM for optimal performance

### Environment Setup

```bash
# Install conda if not already installed
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
bash miniconda.sh -b -p $HOME/miniconda   # -b = batch (no prompts)
eval "$($HOME/miniconda/bin/conda shell.bash hook)"  # add conda command to shell
conda init      # write to ~/.bashrc then open new shell or source ~/.bashrc

# Create conda environment
conda create -n clickbait python=3.10 -y
conda activate clickbait

# Deactivate virtual environment
deactivate

# Install PyTorch (CUDA 12.1)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Install required packages
pip install -r requirements.txt

# Install PEFT for LoRA training
pip install peft bitsandbytes accelerate
```

### Hugging Face Authentication

For accessing gated models (Mistral, Llama):

```bash
# Login to Hugging Face
huggingface-cli login

# Or set environment variable
export HUGGINGFACE_HUB_TOKEN="your_token_here"
```

## ğŸ“ Project Structure

```
clickbait-classification-LLM/
â”œâ”€â”€ ğŸ”¬ fine-tuning/              # Fine-tuning module
â”‚   â”œâ”€â”€ scripts/                 # Training scripts
â”‚   â”‚   â”œâ”€â”€ train_llm_lora.py    # Fine-tune LLMs with LoRA
â”‚   â”‚   â”œâ”€â”€ train_bert_family.py # Fine-tune BERT family models
â”‚   â”‚   â””â”€â”€ evaluate_model.py    # Model evaluation
â”‚   â”œâ”€â”€ outputs/                 # Training results (checkpoints, logs)
â”‚   â”œâ”€â”€ models/                  # Fine-tuned models
â”‚   â””â”€â”€ README.md                # Fine-tuning guide
â”œâ”€â”€ ğŸ¯ prompting/                # Prompting module
â”‚   â”œâ”€â”€ scripts/                 # Prompting scripts
â”‚   â”‚   â”œâ”€â”€ prompting_example.py     # Basic prompting examples
â”‚   â”‚   â”œâ”€â”€ prompting_unified.py     # Unified prompting approach
â”‚   â”‚   â”œâ”€â”€ improved_prompting.py    # Advanced prompting techniques
â”‚   â”‚   â””â”€â”€ inference.py            # Inference with prompting
â”‚   â”œâ”€â”€ outputs/                 # Prompting results
â”‚   â”œâ”€â”€ results/                 # Evaluation results
â”‚   â””â”€â”€ README.md                # Prompting guide
â”œâ”€â”€ ğŸ”§ shared/                   # Shared resources
â”‚   â”œâ”€â”€ utils/                   # Utility functions
â”‚   â”‚   â”œâ”€â”€ utils.py             # General utilities
â”‚   â”‚   â”œâ”€â”€ data_preprocessor.py # Data preprocessing
â”‚   â”‚   â”œâ”€â”€ data_analysis.py     # Data analysis tools
â”‚   â”‚   â””â”€â”€ preprocess_clickbait_alpaca.py # Alpaca format preprocessing
â”‚   â”œâ”€â”€ scripts/                 # Common scripts
â”‚   â”‚   â”œâ”€â”€ setup_environment.py     # Environment setup
â”‚   â”‚   â”œâ”€â”€ run_all_experiments.py   # Run all experiments
â”‚   â”‚   â”œâ”€â”€ benchmark_results.py     # Benchmark results
â”‚   â”‚   â”œâ”€â”€ run_evaluation.py        # Common evaluation
â”‚   â”‚   â”œâ”€â”€ test_api.py              # API testing
â”‚   â”‚   â”œâ”€â”€ test_api_unified.py      # Unified API testing
â”‚   â”‚   â””â”€â”€ test_available_models.py # Model availability testing
â”‚   â””â”€â”€ data/                    # Datasets
â”‚       â”œâ”€â”€ train/               # Training data
â”‚       â”œâ”€â”€ val/                 # Validation data
â”‚       â””â”€â”€ test/                # Testing data
â”œâ”€â”€ ğŸ“š docs/                     # Documentation
â”‚   â””â”€â”€ PROMPTING_GUIDE.md       # Prompting techniques guide
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ README.md                    # This file
â””â”€â”€ .gitignore                   # Git ignore rules
```

## ğŸ¤– Supported Models

### BERT Family Models

| Model | Batch Size | Learning Rate | Epochs | Max Length | Training Time |
|-------|------------|---------------|--------|------------|---------------|
| BERT-base-uncased | 96 | 2e-5 | 4 | 128 | ~45 min |
| BERT-large-uncased | 32 | 1e-5 | 3 | 128 | ~1.5 hours |

### Large Language Models (LoRA)

| Model | Quantization | LoRA Rank | Batch Size | Training Time |
|-------|--------------|-----------|------------|---------------|
| Mistral-7B-Instruct-v0.3 | 4-bit | 8 | 20 | ~2 hours |
| Llama-3.1-8B-Instruct | 4-bit | 8 | 16 | ~3 hours |

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Setup environment and dependencies
python shared/scripts/setup_environment.py

# Run all experiments (both fine-tuning and prompting)
python shared/scripts/run_all_experiments.py
```

### 2. Fine-tuning Approach

```bash
# Train BERT family models
python fine-tuning/scripts/train_bert_family.py --model bert-base-uncased
python fine-tuning/scripts/train_bert_family.py --model bert-large-uncased

# Train LLM with LoRA (ensure Hugging Face authentication first)
huggingface-cli login
python fine-tuning/scripts/train_llm_lora.py --model mistral-7b-instruct
python fine-tuning/scripts/train_llm_lora.py --model llama3-8b

# Evaluate fine-tuned models
python fine-tuning/scripts/evaluate_model.py --model_path fine-tuning/models/bert-base-uncased
```

### 3. Prompting Approach

```bash
# Basic prompting examples
python prompting/scripts/prompting_example.py

# Unified prompting approach
python prompting/scripts/prompting_unified.py

# Advanced prompting techniques
python prompting/scripts/improved_prompting.py

# Inference with prompting
python prompting/scripts/inference.py --text "You won't believe what happened next!"
```

### 4. Benchmark and Compare Results

```bash
# Compare results between fine-tuning and prompting
python shared/scripts/benchmark_results.py

# Run comprehensive evaluation
python shared/scripts/run_evaluation.py
```

## ğŸ“Š Performance Results

### BERT Models

- **BERT-base-uncased**: 
  - Accuracy: 83.2%
  - F1-score: 85.1%
  - Training time: 45 minutes
  
- **BERT-large-uncased**:
  - Accuracy: 85.7%
  - F1-score: 87.3%
  - Training time: 1.5 hours

### LLM Models (LoRA)

- **Mistral-7B-Instruct-v0.3**:
  - Accuracy: 87.9%
  - F1-score: 89.2%
  - Training time: 2 hours
  - Parameters trained: ~0.5% of total

- **Llama-3.1-8B-Instruct**:
  - Accuracy: 88.5%
  - F1-score: 90.1%
  - Training time: 3 hours
  - Parameters trained: ~0.5% of total

## ğŸ”§ Technical Fixes Implemented

### 1. PyTorch Security Issue
- **Problem**: DeBERTa model blocked due to PyTorch vulnerability (CVE-2025-32434)
- **Solution**: 
  - Updated PyTorch to 2.5.1+
  - Removed DeBERTa from training pipeline
  - Focus on stable BERT models

### 2. Transformers API Compatibility
- **Problem**: `evaluation_strategy` parameter deprecated
- **Solution**: Updated to `eval_strategy` for newer transformers versions

### 3. Padding Token Issues
- **Problem**: LLM models missing padding tokens causing batch processing errors
- **Solution**:
  ```python
  if tokenizer.pad_token is None:
      tokenizer.pad_token = tokenizer.eos_token
      tokenizer.pad_token_id = tokenizer.eos_token_id
  tokenizer.padding_side = "right"
  ```

### 4. Memory Optimization
- **Techniques used**:
  - Gradient checkpointing
  - FP16/BF16 mixed precision
  - Gradient accumulation
  - 4-bit/8-bit quantization for LLMs

## ğŸ› Troubleshooting

### Common Issues

#### GPU Memory Errors
```bash
# Reduce batch size in model configs
# Enable gradient checkpointing
# Use mixed precision training
```

#### Hugging Face Authentication
```bash
# Check login status
huggingface-cli whoami

# Re-login if needed
huggingface-cli logout
huggingface-cli login
```

#### Import Errors
```bash
# Install missing dependencies
pip install transformers datasets torch
pip install accelerate bitsandbytes peft
pip install scikit-learn pandas numpy
```

#### Data Loading Issues
```bash
# Verify data files exist
ls -la data/train/data.jsonl
ls -la data/val/data.jsonl
ls -la data/test/data.jsonl
```

## ğŸ“ˆ Monitoring & Logging

### Training Monitoring
- **Weights & Biases**: Automatic logging of metrics
- **Tensorboard**: Local training visualization
- **Console output**: Real-time training progress

### Log Locations
- Training logs: `outputs/{model_name}/runs/`
- Model checkpoints: `outputs/{model_name}/checkpoint-*/`
- Results: `outputs/{model_name}/results.json`

## ğŸ” Model Configuration Details

### BERT Training Arguments
```python
TrainingArguments(
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=48,
    num_train_epochs=4,
    weight_decay=0.01,
    warmup_steps=500,
    fp16=True,
    gradient_checkpointing=True,
    load_best_model_at_end=True,
    metric_for_best_model="f1"
)
```

### LoRA Configuration
```python
LoraConfig(
    task_type=TaskType.SEQ_CLS,
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    bias="none"
)
```

## ğŸ¯ Future Improvements

### Planned Features

1. **Prompting Approaches**
   - Zero-shot classification with GPT-4
   - Few-shot learning with Claude
   - Chain-of-thought prompting

2. **Ensemble Methods**
   - Model averaging
   - Voting classifiers
   - Stacking approaches

3. **Data Augmentation**
   - Paraphrasing with T5
   - Back-translation
   - Synthetic data generation

4. **Advanced Techniques**
   - Adversarial training
   - Knowledge distillation
   - Multi-task learning

## ğŸ“‹ Requirements

### Python Dependencies
See `requirements.txt` for complete list.

### Hardware Requirements
- **Minimum**: 16GB VRAM GPU
- **Recommended**: High-end GPU with 16GB+ VRAM
- **RAM**: 32GB+ system memory
- **Storage**: 50GB+ free space

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸš€ Push to GitHub (Repo Owner)

Direct push method for repository owners:

### 1. Check for existing SSH keys
```bash
ls ~/.ssh/id_ed25519.pub  # if no file exists, create new one below
```

### 2. Generate SSH key pair (ED25519 - strong and short)
```bash
ssh-keygen -t ed25519 -C "your-email@example.com"    # press Enter 3 times for defaults
# Creates ~/.ssh/id_ed25519 & id_ed25519.pub
```

### 3. Add key to ssh-agent (helps git not ask for passphrase every time)
```bash
eval "$(ssh-agent -s)"
ssh-add ~/.ssh/id_ed25519
```

### 4. Copy public key
```bash
cat ~/.ssh/id_ed25519.pub
```

### 5. Add to GitHub
- Go to Settings â†’ Deploy keys â†’ Add deploy key
- Enter title and paste key into text area â†’ Add key

### 6. Test connection in project terminal
```bash
ssh -T git@github.com
# First time will ask "Are you sure you want to continue connecting?" â†’ type yes
# Should see message: "Hi <username>! You've successfully authenticated..."
```

### 7. Point remote to SSH instead of HTTPS
```bash
# Check current remote (should be https)
git remote -v

# Change to SSH
git remote set-url origin git@github.com:<username>/<repo>.git
```

### 8. Now you can push without password prompts
```bash
git add .
git commit -m "Fix BERT training"
git push origin main      # won't ask for user/password anymore
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Webis-Clickbait-17 dataset creators
- Hugging Face for transformers library
- Microsoft for PEFT library
- The open-source ML community

## ğŸ“ Support

For questions and support:
- Create an issue in this repository
- Check the troubleshooting section
- Review the documentation in `docs/`

---

**Note**: This project is optimized for high-end CUDA GPUs. Adjust batch sizes and configurations for different hardware setups.
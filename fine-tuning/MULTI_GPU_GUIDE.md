# üöÄ H∆∞·ªõng d·∫´n c·∫•u h√¨nh v√† ch·∫°y 2x GPU RTX A6000

## üìã T·ªïng quan

H∆∞·ªõng d·∫´n n√†y s·∫Ω gi√∫p b·∫°n c·∫•u h√¨nh v√† ch·∫°y distributed training tr√™n 2 GPU RTX A6000 cho vi·ªác fine-tune c√°c m√¥ h√¨nh Mistral v√† Llama.

## üîß B∆∞·ªõc 1: Ki·ªÉm tra v√† Setup Environment

### 1.1 Ch·∫°y script setup t·ª± ƒë·ªông
```bash
# Ch·∫°y script ki·ªÉm tra v√† setup
./fine-tuning/scripts/setup_multi_gpu.sh
```

### 1.2 Ki·ªÉm tra th·ªß c√¥ng (n·∫øu c·∫ßn)

#### Ki·ªÉm tra GPU:
```bash
nvidia-smi
# Ph·∫£i th·∫•y 2 GPU RTX A6000, m·ªói GPU c√≥ ~48GB VRAM
```

#### Ki·ªÉm tra CUDA:
```bash
nvcc --version
# C·∫ßn CUDA 11.8+ ho·∫∑c 12.x
```

#### Ki·ªÉm tra PyTorch v·ªõi GPU:
```bash
conda activate clickbait
python -c "
import torch
print(f'PyTorch version: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
print(f'GPU count: {torch.cuda.device_count()}')
for i in range(torch.cuda.device_count()):
    print(f'GPU {i}: {torch.cuda.get_device_name(i)}')
"
```

## üéØ B∆∞·ªõc 2: C√°c c√°ch ch·∫°y Training

### 2.1 C√°ch 1: S·ª≠ d·ª•ng script c√≥ s·∫µn (ƒê∆°n gi·∫£n nh·∫•t)

```bash
# K√≠ch ho·∫°t environment
conda activate clickbait

# Ch·∫°y c·∫£ 2 models
./fine-tuning/scripts/run_distributed_training.sh

# Ho·∫∑c ch·ªâ ch·∫°y 1 model
./fine-tuning/scripts/run_distributed_training.sh mistral-7b-instruct
./fine-tuning/scripts/run_distributed_training.sh llama3-8b
```

### 2.2 C√°ch 2: Ch·∫°y th·ªß c√¥ng v·ªõi torch.distributed.launch

```bash
conda activate clickbait

# Set environment variables
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export CUDA_LAUNCH_BLOCKING=0
export NCCL_P2P_DISABLE=0
export NCCL_IB_DISABLE=1

# Ch·∫°y distributed training
python -m torch.distributed.launch \
    --nproc_per_node=2 \
    --master_port=29500 \
    fine-tuning/scripts/train_llm_lora.py \
    --model all \
    --output_dir fine-tuning/outputs
```

### 2.3 C√°ch 3: S·ª≠ d·ª•ng Accelerate (Alternative)

```bash
# C√†i ƒë·∫∑t accelerate n·∫øu ch∆∞a c√≥
pip install accelerate

# Config accelerate (ch·ªâ c·∫ßn l√†m 1 l·∫ßn)
accelerate config
# Ch·ªçn:
# - Distributed training: Yes
# - Number of machines: 1
# - Number of processes: 2
# - GPU IDs: 0,1
# - Mixed precision: bf16

# Ch·∫°y training
accelerate launch fine-tuning/scripts/train_llm_lora.py --model all
```

## ‚öôÔ∏è B∆∞·ªõc 3: Gi·∫£i th√≠ch c√°c tham s·ªë quan tr·ªçng

### Environment Variables:
```bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True  # T·ªëi ∆∞u memory allocation
export CUDA_LAUNCH_BLOCKING=0                           # TƒÉng performance
export NCCL_P2P_DISABLE=0                              # Enable peer-to-peer GPU communication
export NCCL_IB_DISABLE=1                               # Disable InfiniBand (kh√¥ng c·∫ßn cho local)
export NCCL_DEBUG=INFO                                 # Debug info cho NCCL
```

### Training Arguments quan tr·ªçng:
- `--nproc_per_node=2`: S·ªë GPU s·ª≠ d·ª•ng
- `--master_port=29500`: Port cho distributed training
- `ddp_backend="nccl"`: Backend t·ªëi ∆∞u cho NVIDIA GPUs
- `bf16=True`: Mixed precision cho RTX A6000

## üìä B∆∞·ªõc 4: Monitor Training

### 4.1 Theo d√µi real-time:
```bash
# Xem GPU usage
watch -n 1 nvidia-smi

# Xem logs
tail -f fine-tuning/outputs/*/logs/*.log
```

### 4.2 Ki·ªÉm tra progress:
Training s·∫Ω t·∫°o c√°c files:
- `fine-tuning/outputs/mistral-7b-instruct-lora-cuda/`
- `fine-tuning/outputs/llama3-8b-lora-cuda/`
- `fine-tuning/outputs/llm_lora_summary.json`

## üîç B∆∞·ªõc 5: Troubleshooting

### 5.1 L·ªói common v√† c√°ch fix:

#### "CUDA out of memory":
```bash
# Gi·∫£m batch size trong config
# Mistral: batch_size t·ª´ 8 ‚Üí 4
# Llama: batch_size t·ª´ 16 ‚Üí 8
```

#### "NCCL initialization failed":
```bash
# Ki·ªÉm tra GPUs c√≥ ƒë∆∞·ª£c nh·∫≠n di·ªán kh√¥ng
nvidia-smi

# Restart v√† th·ª≠ l·∫°i
sudo nvidia-smi -r
```

#### "Port already in use":
```bash
# ƒê·ªïi port kh√°c
--master_port=29501
```

### 5.2 Debug commands:
```bash
# Test distributed setup
python -c "
import torch
import torch.distributed as dist
print('Testing distributed setup...')
"

# Check NCCL
python -c "
import torch
print('NCCL available:', torch.distributed.is_nccl_available())
"
```

## üìà B∆∞·ªõc 6: Expected Performance

### Memory Usage (per GPU):
- **Mistral-7B**: ~40GB VRAM
- **Llama-8B**: ~45GB VRAM

### Training Time:
- **Mistral**: ~3-4 gi·ªù (5 epochs)
- **Llama**: ~4-5 gi·ªù (5 epochs)
- **Total**: ~6-8 gi·ªù cho c·∫£ 2 models

### Effective Batch Sizes:
- **Mistral**: 8 (per GPU) √ó 4 (accum) √ó 2 (GPUs) = 64
- **Llama**: 16 (per GPU) √ó 4 (accum) √ó 2 (GPUs) = 128

## üéØ Quick Start Commands

```bash
# All-in-one setup and training
cd /path/to/your/project
conda activate clickbait
./fine-tuning/scripts/setup_multi_gpu.sh
./fine-tuning/scripts/run_distributed_training.sh

# Monitor
watch -n 1 nvidia-smi
```

## üìû Support

N·∫øu g·∫∑p v·∫•n ƒë·ªÅ:
1. Check `fine-tuning/outputs/*/logs/` ƒë·ªÉ xem error logs
2. ƒê·∫£m b·∫£o CUDA drivers updated
3. Ki·ªÉm tra VRAM availability v·ªõi `nvidia-smi`
4. Th·ª≠ ch·∫°y single GPU tr∆∞·ªõc ƒë·ªÉ test setup 
# Cáº¥u TrÃºc Dá»± Ãn: Khai thÃ¡c LLM Ä‘á»ƒ PhÃ¡t hiá»‡n Clickbait

## ğŸ“ Cáº¥u TrÃºc ThÆ° Má»¥c

```
clickbait-classification/
â”‚
â”œâ”€â”€ ğŸ“ data/                              # Dá»¯ liá»‡u thÃ´ vÃ  Ä‘Ã£ xá»­ lÃ½
â”‚   â”œâ”€â”€ raw/                              # Dá»¯ liá»‡u gá»‘c tá»« Webis-Clickbait-17
â”‚   â”œâ”€â”€ processed/                        # Dá»¯ liá»‡u Ä‘Ã£ tiá»n xá»­ lÃ½
â”‚   â””â”€â”€ splits/                           # Dá»¯ liá»‡u Ä‘Ã£ chia train/val/test
â”‚
â”œâ”€â”€ ğŸ“ src/                               # Source code chÃ­nh
â”‚   â”œâ”€â”€ ğŸ“ data_processing/               # Xá»­ lÃ½ dá»¯ liá»‡u
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_loader.py               # Load dá»¯ liá»‡u Webis-Clickbait-17
â”‚   â”‚   â”œâ”€â”€ text_cleaner.py              # LÃ m sáº¡ch vÄƒn báº£n social media
â”‚   â”‚   â”œâ”€â”€ data_splitter.py             # Chia dá»¯ liá»‡u train/val/test
â”‚   â”‚   â””â”€â”€ label_converter.py           # Chuyá»ƒn Ä‘á»•i truthMean -> binary
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ fine_tuning/                  # Fine-tuning approach
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ models/                      # CÃ¡c mÃ´ hÃ¬nh há»— trá»£
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ bert_model.py           # BERT/RoBERTa fine-tuning
â”‚   â”‚   â”‚   â”œâ”€â”€ mistral_model.py        # Mistral-7B fine-tuning
â”‚   â”‚   â”‚   â””â”€â”€ llama_model.py          # LLaMA fine-tuning
â”‚   â”‚   â”œâ”€â”€ trainers/                   # Training utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ base_trainer.py         # Base trainer class
â”‚   â”‚   â”‚   â”œâ”€â”€ peft_trainer.py         # PEFT/LoRA trainer
â”‚   â”‚   â”‚   â””â”€â”€ unsloth_trainer.py      # Unsloth optimized trainer
â”‚   â”‚   â”œâ”€â”€ tokenizers/                 # Tokenization utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ advanced_tokenizer.py  # Specialized tokenizers
â”‚   â”‚   â””â”€â”€ evaluation/                 # Evaluation utilities
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ metrics.py              # Metrics calculation
â”‚   â”‚       â””â”€â”€ visualizations.py      # Result visualization
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ prompting/                   # Prompting approach
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ prompt_templates/           # Prompt designs
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ zero_shot.py           # Zero-shot prompts
â”‚   â”‚   â”‚   â””â”€â”€ few_shot.py            # Few-shot prompts
â”‚   â”‚   â”œâ”€â”€ llm_clients/               # LLM API clients
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ openai_client.py       # GPT-4/GPT-4o
â”‚   â”‚   â”‚   â”œâ”€â”€ anthropic_client.py    # Claude 3
â”‚   â”‚   â”‚   â””â”€â”€ huggingface_client.py  # Open-source LLMs
â”‚   â”‚   â””â”€â”€ evaluation/                # Prompting evaluation
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ batch_evaluator.py     # Batch evaluation
â”‚   â”‚       â””â”€â”€ cost_calculator.py     # API cost calculation
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ utils/                      # Utilities chung
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py                  # Configuration management
â”‚   â”‚   â”œâ”€â”€ logging_utils.py           # Logging utilities
â”‚   â”‚   â”œâ”€â”€ gpu_utils.py               # GPU management
â”‚   â”‚   â””â”€â”€ file_utils.py              # File I/O utilities
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ comparison/                 # So sÃ¡nh hai approach
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ benchmark.py               # Benchmark comparison
â”‚       â”œâ”€â”€ cost_analysis.py           # Cost analysis
â”‚       â””â”€â”€ recommendation.py         # Final recommendations
â”‚
â”œâ”€â”€ ğŸ“ notebooks/                      # Jupyter notebooks cho EDA
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb      # KhÃ¡m phÃ¡ dá»¯ liá»‡u
â”‚   â”œâ”€â”€ 02_text_cleaning_analysis.ipynb # PhÃ¢n tÃ­ch text cleaning
â”‚   â”œâ”€â”€ 03_fine_tuning_experiments.ipynb # ThÃ­ nghiá»‡m fine-tuning
â”‚   â”œâ”€â”€ 04_prompting_experiments.ipynb # ThÃ­ nghiá»‡m prompting
â”‚   â””â”€â”€ 05_final_comparison.ipynb      # So sÃ¡nh cuá»‘i cÃ¹ng
â”‚
â”œâ”€â”€ ğŸ“ configs/                        # Configuration files
â”‚   â”œâ”€â”€ data_config.yaml              # Data processing configs
â”‚   â”œâ”€â”€ model_config.yaml             # Model configurations
â”‚   â”œâ”€â”€ training_config.yaml          # Training parameters
â”‚   â””â”€â”€ api_config.yaml               # API configurations
â”‚
â”œâ”€â”€ ğŸ“ scripts/                        # Scripts cháº¡y tá»«ng bÆ°á»›c
â”‚   â”œâ”€â”€ 01_setup_environment.sh        # Setup mÃ´i trÆ°á»ng
â”‚   â”œâ”€â”€ 02_download_data.py            # Download dá»¯ liá»‡u
â”‚   â”œâ”€â”€ 03_process_data.py             # Xá»­ lÃ½ dá»¯ liá»‡u
â”‚   â”œâ”€â”€ 04_fine_tune_models.py         # Fine-tuning pipeline
â”‚   â”œâ”€â”€ 05_run_prompting.py            # Prompting pipeline
â”‚   â””â”€â”€ 06_compare_results.py          # So sÃ¡nh káº¿t quáº£
â”‚
â”œâ”€â”€ ğŸ“ experiments/                    # Káº¿t quáº£ thÃ­ nghiá»‡m
â”‚   â”œâ”€â”€ fine_tuning/                   # Káº¿t quáº£ fine-tuning
â”‚   â”‚   â”œâ”€â”€ roberta_base/              # RoBERTa results
â”‚   â”‚   â”œâ”€â”€ mistral_7b/                # Mistral results
â”‚   â”‚   â””â”€â”€ llama_3_8b/                # LLaMA results
â”‚   â”œâ”€â”€ prompting/                     # Káº¿t quáº£ prompting
â”‚   â”‚   â”œâ”€â”€ gpt4_zero_shot/            # GPT-4 zero-shot
â”‚   â”‚   â”œâ”€â”€ gpt4_few_shot/             # GPT-4 few-shot
â”‚   â”‚   â”œâ”€â”€ claude3_zero_shot/         # Claude 3 zero-shot
â”‚   â”‚   â””â”€â”€ claude3_few_shot/          # Claude 3 few-shot
â”‚   â””â”€â”€ comparison/                    # So sÃ¡nh tá»•ng thá»ƒ
â”‚       â”œâ”€â”€ performance_metrics.json   # Metrics comparison
â”‚       â”œâ”€â”€ cost_analysis.json         # Cost comparison
â”‚       â””â”€â”€ visualizations/            # Charts and plots
â”‚
â”œâ”€â”€ ğŸ“ models/                         # Saved models
â”‚   â”œâ”€â”€ fine_tuned/                    # Fine-tuned models
â”‚   â””â”€â”€ checkpoints/                   # Training checkpoints
â”‚
â”œâ”€â”€ ğŸ“ outputs/                        # Output files
â”‚   â”œâ”€â”€ predictions/                   # Model predictions
â”‚   â”œâ”€â”€ reports/                       # Generated reports
â”‚   â””â”€â”€ logs/                          # Log files
â”‚
â”œâ”€â”€ ğŸ“ tests/                          # Unit tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_data_processing.py        # Test data processing
â”‚   â”œâ”€â”€ test_fine_tuning.py            # Test fine-tuning
â”‚   â””â”€â”€ test_prompting.py              # Test prompting
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt                # Python dependencies
â”œâ”€â”€ ğŸ“„ environment.yml                 # Conda environment
â”œâ”€â”€ ğŸ“„ setup.py                        # Package setup
â”œâ”€â”€ ğŸ“„ README.md                       # Project overview
â”œâ”€â”€ ğŸ“„ INSTALLATION.md                 # Installation guide
â”œâ”€â”€ ğŸ“„ USAGE.md                        # Usage instructions
â””â”€â”€ ğŸ“„ RESULTS.md                      # Final results summary
```

## ğŸ¯ Workflow Tá»•ng Thá»ƒ

### Phase 1: Data Processing
1. Download vÃ  extract Webis-Clickbait-17
2. PhÃ¢n tÃ­ch vÃ  lÃ m sáº¡ch dá»¯ liá»‡u
3. Chuyá»ƒn Ä‘á»•i labels vÃ  chia train/val/test
4. Xá»­ lÃ½ class imbalance

### Phase 2: Fine-tuning Approach
1. ThÃ­ nghiá»‡m vá»›i RoBERTa-base (baseline)
2. Fine-tuning Mistral-7B vá»›i PEFT/LoRA
3. Fine-tuning LLaMA-3-8B vá»›i Unsloth
4. Evaluation vÃ  optimization

### Phase 3: Prompting Approach  
1. Thiáº¿t káº¿ zero-shot prompts
2. Thiáº¿t káº¿ few-shot prompts
3. Test vá»›i GPT-4/GPT-4o
4. Test vá»›i Claude 3
5. Test vá»›i open-source LLMs

### Phase 4: Comparison & Analysis
1. Performance comparison
2. Cost analysis
3. Speed benchmarking
4. Resource requirements
5. Final recommendations

## ğŸ”§ Tech Stack

### Core Libraries
- **Data Processing**: pandas, numpy, datasets
- **Machine Learning**: scikit-learn, transformers
- **Deep Learning**: torch, accelerate
- **Optimization**: peft, unsloth, bitsandbytes
- **Evaluation**: seaborn, matplotlib, wandb
- **APIs**: openai, anthropic, huggingface_hub

### Infrastructure
- **GPU**: A5000 (Ä‘á»ƒ training)
- **Environment**: conda/venv
- **Monitoring**: wandb, tensorboard
- **Storage**: HuggingFace Hub, local storage 
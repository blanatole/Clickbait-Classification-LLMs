#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Data Insights Summary - T√≥m t·∫Øt ph√¢n t√≠ch d·ªØ li·ªáu clickbait
"""

import json
import os

def load_analysis_results(file_path="data_analysis_results.json"):
    """Load analysis results from JSON file"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"‚ùå Error loading analysis results: {e}")
        return None

def print_key_insights(analysis):
    """Print key insights from the analysis"""
    
    print("üîç PH√ÇN T√çCH D·ªÆ LI·ªÜU CLICKBAIT - K·∫æT QU·∫¢ CHI TI·∫æT")
    print("=" * 60)
    
    # Overall statistics
    if 'overall_statistics' in analysis:
        overall = analysis['overall_statistics']['basic_statistics']
        print(f"\nüìä TH·ªêNG K√ä T·ªîNG QUAN:")
        print(f"   ‚Ä¢ T·ªïng s·ªë m·∫´u: {overall['total_samples']:,}")
        print(f"   ‚Ä¢ Clickbait: {overall['class_distribution']['clickbait']['count']:,} ({overall['class_distribution']['clickbait']['percentage']}%)")
        print(f"   ‚Ä¢ No-clickbait: {overall['class_distribution']['no_clickbait']['count']:,} ({overall['class_distribution']['no_clickbait']['percentage']}%)")
        print(f"   ‚Ä¢ ƒê·ªô d√†i text trung b√¨nh: {overall['text_length_statistics']['mean']} k√Ω t·ª±")
        print(f"   ‚Ä¢ ƒê·ªô d√†i text ng·∫Øn nh·∫•t: {overall['text_length_statistics']['min']} k√Ω t·ª±")
        print(f"   ‚Ä¢ ƒê·ªô d√†i text d√†i nh·∫•t: {overall['text_length_statistics']['max']} k√Ω t·ª±")
    
    # Text patterns comparison
    if 'overall_statistics' in analysis:
        text_patterns = analysis['overall_statistics']['text_patterns']
        
        print(f"\nüìù SO S√ÅNH PATTERNS TEXT:")
        
        clickbait = text_patterns['clickbait_patterns']
        no_clickbait = text_patterns['no_clickbait_patterns']
        
        print(f"   CLICKBAIT:")
        print(f"   ‚Ä¢ ƒê·ªô d√†i trung b√¨nh: {clickbait['average_length']:.1f} k√Ω t·ª±")
        print(f"   ‚Ä¢ S·ªë t·ª´ trung b√¨nh: {clickbait['average_word_count']:.1f} t·ª´")
        print(f"   ‚Ä¢ C√≥ d·∫•u h·ªèi: {clickbait['punctuation_stats']['question_marks']['percentage']:.1f}%")
        print(f"   ‚Ä¢ C√≥ d·∫•u c·∫£m: {clickbait['punctuation_stats']['exclamation_marks']['percentage']:.1f}%")
        
        print(f"   NO-CLICKBAIT:")
        print(f"   ‚Ä¢ ƒê·ªô d√†i trung b√¨nh: {no_clickbait['average_length']:.1f} k√Ω t·ª±")
        print(f"   ‚Ä¢ S·ªë t·ª´ trung b√¨nh: {no_clickbait['average_word_count']:.1f} t·ª´")
        print(f"   ‚Ä¢ C√≥ d·∫•u h·ªèi: {no_clickbait['punctuation_stats']['question_marks']['percentage']:.1f}%")
        print(f"   ‚Ä¢ C√≥ d·∫•u c·∫£m: {no_clickbait['punctuation_stats']['exclamation_marks']['percentage']:.1f}%")
    
    # Top discriminative keywords
    if 'overall_statistics' in analysis:
        keywords = analysis['overall_statistics']['keyword_analysis']['top_discriminative_keywords']
        
        print(f"\nüîë T·ª™ KH√ìA PH√ÇN BI·ªÜT CLICKBAIT:")
        for i, (keyword, ratio) in enumerate(list(keywords.items())[:10], 1):
            if ratio != float('inf'):
                print(f"   {i:2d}. '{keyword}': t·ª∑ l·ªá {ratio:.2f}x (xu·∫•t hi·ªán nhi·ªÅu h∆°n trong clickbait)")
    
    # Advanced patterns
    if 'overall_statistics' in analysis:
        advanced = analysis['overall_statistics']['advanced_patterns']
        
        print(f"\nüéØ PATTERNS N√ÇNG CAO:")
        
        cb_patterns = advanced['clickbait_advanced_patterns']
        ncb_patterns = advanced['no_clickbait_advanced_patterns']
        
        print(f"   CLICKBAIT:")
        print(f"   ‚Ä¢ C√≥ s·ªë: {cb_patterns['formatting_patterns']['has_numbers']['percentage']:.1f}%")
        print(f"   ‚Ä¢ C√≥ t·ª´ so s√°nh: {cb_patterns['linguistic_patterns']['superlatives']['percentage']:.1f}%")
        print(f"   ‚Ä¢ C√≥ t·ª´ th·ªùi gian: {cb_patterns['linguistic_patterns']['time_words']['percentage']:.1f}%")
        print(f"   ‚Ä¢ C√≥ ƒë·∫°i t·ª´ nh√¢n x∆∞ng: {cb_patterns['linguistic_patterns']['personal_pronouns']['percentage']:.1f}%")
        
        print(f"   NO-CLICKBAIT:")
        print(f"   ‚Ä¢ C√≥ s·ªë: {ncb_patterns['formatting_patterns']['has_numbers']['percentage']:.1f}%")
        print(f"   ‚Ä¢ C√≥ t·ª´ so s√°nh: {ncb_patterns['linguistic_patterns']['superlatives']['percentage']:.1f}%")
        print(f"   ‚Ä¢ C√≥ t·ª´ th·ªùi gian: {ncb_patterns['linguistic_patterns']['time_words']['percentage']:.1f}%")
        print(f"   ‚Ä¢ C√≥ ƒë·∫°i t·ª´ nh√¢n x∆∞ng: {ncb_patterns['linguistic_patterns']['personal_pronouns']['percentage']:.1f}%")
    
    # Truth score analysis
    if 'overall_statistics' in analysis:
        truth_analysis = analysis['overall_statistics']['truth_score_analysis']['truth_score_distribution']
        
        print(f"\nüìà PH√ÇN T√çCH TRUTH SCORES:")
        for score_range, data in truth_analysis.items():
            print(f"   {score_range}: {data['total_samples']} m·∫´u, {data['clickbait_percentage']:.1f}% clickbait")
    
    # Comparative insights
    if 'comparative_analysis' in analysis:
        comp = analysis['comparative_analysis']
        
        print(f"\nüîÑ INSIGHTS SO S√ÅNH:")
        print(f"   ‚Ä¢ Clickbait ng·∫Øn h∆°n {abs(comp['key_differences']['average_length_difference']):.1f} k√Ω t·ª±")
        print(f"   ‚Ä¢ Clickbait: {comp['key_differences']['clickbait_avg_length']:.1f} k√Ω t·ª±")
        print(f"   ‚Ä¢ No-clickbait: {comp['key_differences']['no_clickbait_avg_length']:.1f} k√Ω t·ª±")
    
    # Top words in each class
    if 'overall_statistics' in analysis:
        text_patterns = analysis['overall_statistics']['text_patterns']
        
        print(f"\nüìä TOP T·ª™ XU·∫§T HI·ªÜN NHI·ªÄU NH·∫§T:")
        
        print(f"   CLICKBAIT:")
        cb_words = text_patterns['clickbait_patterns']['top_words']
        for i, (word, count) in enumerate(list(cb_words.items())[:10], 1):
            if word != 'sep':  # Skip separator
                print(f"   {i:2d}. '{word}': {count:,} l·∫ßn")
        
        print(f"   NO-CLICKBAIT:")
        ncb_words = text_patterns['no_clickbait_patterns']['top_words']
        for i, (word, count) in enumerate(list(ncb_words.items())[:10], 1):
            if word != 'sep':  # Skip separator
                print(f"   {i:2d}. '{word}': {count:,} l·∫ßn")

def print_recommendations(analysis):
    """Print actionable recommendations"""
    
    print(f"\nüí° KHUY·∫æN NGH·ªä CHO TRAINING MODEL:")
    print("=" * 40)
    
    if 'comparative_analysis' in analysis:
        recommendations = analysis['comparative_analysis']['recommendations']
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")
    
    print(f"\nüéØ FEATURE ENGINEERING SUGGESTIONS:")
    print(f"   ‚Ä¢ Text length (clickbait th∆∞·ªùng ng·∫Øn h∆°n)")
    print(f"   ‚Ä¢ Punctuation ratio (d·∫•u h·ªèi, d·∫•u c·∫£m)")
    print(f"   ‚Ä¢ Personal pronouns count ('you', 'your')")
    print(f"   ‚Ä¢ Question words frequency ('how', 'what', 'why')")
    print(f"   ‚Ä¢ Numbers presence")
    print(f"   ‚Ä¢ Superlative words")
    print(f"   ‚Ä¢ Time urgency words")
    print(f"   ‚Ä¢ TF-IDF scores cho top discriminative words")
    
    print(f"\nüîß MODEL TRAINING TIPS:")
    print(f"   ‚Ä¢ S·ª≠ d·ª•ng attention mechanism cho personal pronouns")
    print(f"   ‚Ä¢ Weight higher cho question marks v√† exclamation marks")
    print(f"   ‚Ä¢ Feature engineering cho text length ratio")
    print(f"   ‚Ä¢ Ensemble method v·ªõi text + metadata features")

def save_insights_summary(analysis, output_file="data_insights_summary.json"):
    """Save key insights to a separate JSON file"""
    
    insights_summary = {
        "timestamp": analysis.get('analysis_metadata', {}).get('timestamp', ''),
        "dataset_overview": {},
        "key_differences": {},
        "top_indicators": {},
        "recommendations": []
    }
    
    # Dataset overview
    if 'overall_statistics' in analysis:
        overall = analysis['overall_statistics']['basic_statistics']
        insights_summary["dataset_overview"] = {
            "total_samples": overall['total_samples'],
            "clickbait_percentage": overall['class_distribution']['clickbait']['percentage'],
            "average_text_length": overall['text_length_statistics']['mean'],
            "text_length_range": {
                "min": overall['text_length_statistics']['min'],
                "max": overall['text_length_statistics']['max']
            }
        }
    
    # Key differences
    if 'overall_statistics' in analysis:
        text_patterns = analysis['overall_statistics']['text_patterns']
        cb_patterns = text_patterns['clickbait_patterns']
        ncb_patterns = text_patterns['no_clickbait_patterns']
        
        insights_summary["key_differences"] = {
            "length_difference": cb_patterns['average_length'] - ncb_patterns['average_length'],
            "question_marks_difference": cb_patterns['punctuation_stats']['question_marks']['percentage'] - 
                                       ncb_patterns['punctuation_stats']['question_marks']['percentage'],
            "exclamation_marks_difference": cb_patterns['punctuation_stats']['exclamation_marks']['percentage'] - 
                                          ncb_patterns['punctuation_stats']['exclamation_marks']['percentage']
        }
    
    # Top indicators
    if 'overall_statistics' in analysis:
        keywords = analysis['overall_statistics']['keyword_analysis']['top_discriminative_keywords']
        insights_summary["top_indicators"] = dict(list(keywords.items())[:10])
    
    # Recommendations
    if 'comparative_analysis' in analysis:
        insights_summary["recommendations"] = analysis['comparative_analysis']['recommendations']
    
    # Save to file
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(insights_summary, f, indent=2, ensure_ascii=False)
        print(f"\nüíæ Insights summary saved to: {output_file}")
        return True
    except Exception as e:
        print(f"‚ùå Error saving insights summary: {e}")
        return False

def main():
    """Main function"""
    
    # Load analysis results
    analysis = load_analysis_results()
    
    if analysis is None:
        print("‚ùå Could not load analysis results!")
        return
    
    # Print insights
    print_key_insights(analysis)
    print_recommendations(analysis)
    
    # Save summary
    save_insights_summary(analysis)
    
    print(f"\n‚úÖ HO√ÄN TH√ÄNH PH√ÇN T√çCH D·ªÆ LI·ªÜU!")
    print(f"üìÅ Files created:")
    print(f"   ‚Ä¢ data_analysis_results.json (full analysis)")
    print(f"   ‚Ä¢ data_insights_summary.json (key insights)")

if __name__ == "__main__":
    main() 